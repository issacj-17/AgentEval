# Evaluation Demo Playbook

This playbook provides a scripted flow for showcasing AgentEval end-to-end during judging or customer demos. It also doubles as a smoke/regression test outline.

## 1. Environment Preparation

1. Ensure staging deployment is online (`https://demo.agenteval.dev`).
2. Verify AWS credentials loaded via IAM Role or AWS Vault.
3. Export environment variables:
   ```bash
   export AGENTEVAL_API_URL=https://demo.agenteval.dev/api/v1
   export AGENTEVAL_API_KEY=<judge-api-key>
   ```

4. Seed sample personas and attack scenarios:
   ```bash
   python scripts/demo_seed.py --personas personas/library.yaml --attacks attacks/catalog.yaml
   ```

## 2. Persona Campaign Smoke Test

| Step | Action | Expected Outcome |
| --- | --- | --- |
| 1 | `python scripts/run_campaign.py --type persona --persona frustrated_customer --target https://postman-echo.com/post` | Campaign ID returned, status `running` |
| 2 | `GET /campaigns/{id}` | Status transitions to `completed` within 2 minutes |
| 3 | `GET /results/{id}` | Summary includes score breakdown & trace links |
| 4 | Export report via UI | PDF received with persona transcript & trace timeline |

## 3. Red Team Campaign Smoke Test

| Step | Action | Expected Outcome |
| --- | --- | --- |
| 1 | Launch red team campaign with `--categories injection,jailbreak` | Attack log created with 5+ attempts |
| 2 | Verify DynamoDB tables updated (`agenteval-turns`, `agenteval-evaluations`) | Items inserted with timestamps |
| 3 | Review EventBridge events via CloudWatch logs | Lifecycle events emitted (`campaign.started`, `campaign.completed`) |

## 4. Trace-Based Root Cause Analysis Highlight

1. Trigger a known failure in demo target (e.g., latency injection).  
2. From UI, open “Trace Correlation” tab and highlight:  
   - Trace ID propagation (`X-Amzn-Trace-Id`)  
   - Service map screenshot  
   - Root cause suggestion generated by `TraceAnalyzer`
3. Capture screenshot/gif for Devpost.

## 5. Automated Test Suite

Run the integration tests (mocked AWS) locally:
```bash
pytest tests/integration/test_campaign_flow.py -v
```
Expected: all tests pass in <90 seconds.  
Note: tests use moto/localstack mocks—ensure they are running or install dev dependencies.

## 6. Demo Narrative (3–4 Minutes)

1. **Problem framing (30s):** Enterprises struggle with GenAI regressions.  
2. **Solution intro (45s):** AgentEval’s multi-agent orchestrator.  
3. **Live walk-through (120s):** Launch persona & red team campaign, show dashboard, highlight root cause view.  
4. **Results & metrics (30s):** Display evaluation summary, scoring.  
5. **Call to action (30s):** Mention next steps, contact info.

## 7. Post-Demo Reset

```bash
python scripts/demo_cleanup.py --campaign <id>
```
Ensures DynamoDB tables and S3 artifacts are cleaned to prepare for the next judge.

## 8. Evidence Capture

- Save terminal logs from script runs.
- Export dashboard screenshots to `req-docs/screenshots/`.
- Record latency metrics (p50/p95/p99) via CloudWatch dashboards and archive as PNG.

Review and update this playbook after each dry run.
